{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip'\n",
    "dataset_path = 'UCI HAR Dataset'\n",
    "\n",
    "# Unzip the dataset\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(url, 'UCI_HAR_Dataset.zip')\n",
    "    with zipfile.ZipFile('UCI_HAR_Dataset.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv(f'{dataset_path}/train/X_train.txt', delim_whitespace=True, header=None)\n",
    "test_data = pd.read_csv(f'{dataset_path}/test/X_test.txt', delim_whitespace=True, header=None)\n",
    "data = pd.concat([train_data, test_data])\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# # Apply DBSCAN\n",
    "# dbscan = DBSCAN(eps=50, min_samples=10)\n",
    "# clusters = dbscan.fit_predict(data_scaled)\n",
    "\n",
    "# # Display the original output of DBSCAN\n",
    "# print(\"Cluster labels for each point in the dataset:\")\n",
    "# print(clusters)\n",
    "\n",
    "# unique, counts = np.unique(clusters, return_counts=True)\n",
    "# cluster_counts = dict(zip(unique, counts))\n",
    "# print(\"\\nNumber of points in each cluster:\")\n",
    "# print(cluster_counts)\n",
    "\n",
    "# # Adding the cluster labels to the dataframe\n",
    "# data['cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "eps: 14, min_samples: 13\n",
      "Number of clusters: 7\n",
      "Silhouette Score: 0.14412940796471713\n",
      "Cluster labels for each point in the dataset:\n",
      "[-1  0  0 ... -1  1  1]\n",
      "\n",
      "Number of points in each cluster:\n",
      "{-1: 3255, 0: 5125, 1: 1791, 2: 34, 3: 13, 4: 25, 5: 20, 6: 36}\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best results\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_clusters = None\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "\n",
    "# Define the grid search ranges for eps and min_samples\n",
    "eps_values = np.arange(1, 100, 1)\n",
    "min_samples_values = np.arange(5, 20, 1)\n",
    "\n",
    "# Function to apply DBSCAN and return the number of clusters\n",
    "def apply_dbscan(eps, min_samples, data):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(data_scaled)\n",
    "    unique_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "    if unique_clusters > 1:  # We need at least 2 clusters to calculate silhouette score\n",
    "        score = silhouette_score(data, clusters)\n",
    "    else:\n",
    "        score = -1\n",
    "    return unique_clusters, score, clusters\n",
    "\n",
    "# Perform the grid search\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        clusters, score, labels = apply_dbscan(eps, min_samples, data_scaled)\n",
    "        if clusters > 6 and clusters < 8 and score > best_score:  # You can also set a specific cluster count target here\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples\n",
    "            best_clusters = clusters\n",
    "            best_score = score\n",
    "            best_labels = labels\n",
    "\n",
    "# Display the best results\n",
    "print(\"Best parameters found:\")\n",
    "print(f\"eps: {best_eps}, min_samples: {best_min_samples}\")\n",
    "print(f\"Number of clusters: {best_clusters}\")\n",
    "print(f\"Silhouette Score: {best_score}\")\n",
    "\n",
    "# Display the original output of DBSCAN with the best parameters\n",
    "print(\"Cluster labels for each point in the dataset:\")\n",
    "print(best_labels)\n",
    "\n",
    "unique, counts = np.unique(best_labels, return_counts=True)\n",
    "cluster_counts = dict(zip(unique, counts))\n",
    "print(\"\\nNumber of points in each cluster:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "# Adding the cluster labels to the dataframe\n",
    "data['cluster'] = best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Clustering Labels for DBSCAN centroids: [1 0 2 0 0 2 0]\n",
      "\n",
      "Hierarchical Cluster labels for each point in the dataset:\n",
      "0      -1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "2942    0\n",
      "2943    0\n",
      "2944   -1\n",
      "2945    0\n",
      "2946    0\n",
      "Name: hierarchical_cluster, Length: 10299, dtype: int64\n",
      "\n",
      "Number of points in each hierarchical cluster:\n",
      "{-1: 3255, 0: 1865, 1: 5125, 2: 54}\n"
     ]
    }
   ],
   "source": [
    "# Calculate centroids of the DBSCAN clusters\n",
    "centroids = []\n",
    "for cluster_id in np.unique(best_labels):\n",
    "    if cluster_id != -1:  # Exclude noise points\n",
    "        cluster_points = data_scaled[best_labels == cluster_id]\n",
    "        centroid = cluster_points.mean(axis=0)\n",
    "        centroids.append(centroid)\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "# Apply hierarchical clustering on the DBSCAN centroids\n",
    "n_clusters = 3  # Set the desired number of clusters for hierarchical clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "hierarchical_labels = hierarchical.fit_predict(centroids)\n",
    "\n",
    "print(\"Hierarchical Clustering Labels for DBSCAN centroids:\", hierarchical_labels)\n",
    "\n",
    "# Map hierarchical cluster labels back to original data points\n",
    "data['hierarchical_cluster'] = -1  # Initialize with -1 (unassigned)\n",
    "for cluster_id, centroid_label in enumerate(hierarchical_labels):\n",
    "    data.loc[data['cluster'] == cluster_id, 'hierarchical_cluster'] = centroid_label\n",
    "\n",
    "# Display the hierarchical cluster labels\n",
    "print(\"\\nHierarchical Cluster labels for each point in the dataset:\")\n",
    "print(data['hierarchical_cluster'])\n",
    "\n",
    "unique, counts = np.unique(data['hierarchical_cluster'], return_counts=True)\n",
    "cluster_counts = dict(zip(unique, counts))\n",
    "print(\"\\nNumber of points in each hierarchical cluster:\")\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sajud_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
